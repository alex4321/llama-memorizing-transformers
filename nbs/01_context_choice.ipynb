{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context choice\n",
    "\n",
    "> The \"Memorizing transformer\" have a module which, for each token, choose between the local (small chunk passed through actual attention) and global (big document) embeddings. This is an implementation of two methods of such a choice - just trainable constant biases (which comes from original paper) and per-head linear classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp context_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "LOCAL_LOSS_K = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def local_score_loss(local_score: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    targets = torch.zeros(local_score.shape, dtype=local_score.dtype, device=local_score.device)\n",
    "    return F.binary_cross_entropy_with_logits(local_score, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseContextChoice(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for every context choice method.\n",
    "    Basically each of these things is some kind of weighted average between the local context and global context.\n",
    "    \"\"\"\n",
    "    def __init__(self, attention_heads: int, embedding_dim: int, loss_k: float = LOCAL_LOSS_K) -> None:\n",
    "        \"\"\"\n",
    "        Initializer.\n",
    "        :param attention_heads: how much attention heads each block of the original transformer have\n",
    "        :param embedding_dim: how much embedding dimensions each block of the original transformer have\n",
    "        \"\"\"\n",
    "        assert attention_heads > 0\n",
    "        assert embedding_dim > 0\n",
    "        assert embedding_dim % attention_heads == 0\n",
    "        super(BaseContextChoice, self).__init__()\n",
    "        self.attention_heads = attention_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_dim = embedding_dim // attention_heads\n",
    "        self._loss_component = 0\n",
    "        self.loss_k = loss_k\n",
    "    \n",
    "    def get_loss_component(self) -> Union[torch.FloatTensor, float]:\n",
    "        result = self._loss_component * self.loss_k\n",
    "        self._loss_component = None\n",
    "        return result\n",
    "    \n",
    "    def forward(self, embeddings_local: torch.FloatTensor, embeddings_global: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Apply the weighted average between embeddings_local and embeddings_global.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each BaseContextChoice subclass must define their own forward method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ContextChoiceLinear(BaseContextChoice):\n",
    "    def __init__(self, attention_heads: int, embedding_dim: int, loss_k: float = LOCAL_LOSS_K) -> None:\n",
    "        super(ContextChoiceLinear, self).__init__(attention_heads, embedding_dim, loss_k)\n",
    "        self.weights = nn.Parameter(torch.randn((self.attention_heads, self.head_dim, 1)))\n",
    "        self.biases = nn.Parameter(torch.randn((self.attention_heads,)))\n",
    "\n",
    "    def forward(self, embeddings_local: torch.FloatTensor, embeddings_global: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        batch_size, sequence_length, _ = embeddings_local.shape\n",
    "        # batch_size x sequence_length x attention_heads x head_dim\n",
    "        embeddings_local = embeddings_local.view((batch_size, sequence_length, self.attention_heads, -1))\n",
    "        embeddings_global = embeddings_global.view((batch_size, sequence_length, self.attention_heads, -1))\n",
    "        # batch_size x sequence_length x attention_heads x 1\n",
    "        # b - batch size\n",
    "        # s - sequence length\n",
    "        # h - attention heads\n",
    "        # d - head dim\n",
    "        # a - 1\n",
    "        local_logits = torch.einsum(\"bshd,hda->bsha\", embeddings_local, self.weights) + self.biases.view((1, 1, self.attention_heads, 1))    \n",
    "        local_score = F.sigmoid(local_logits)\n",
    "        global_score = 1 - local_score\n",
    "        # batch_size x sequence_length x attention_heads x head_dim\n",
    "        embeddings_local_scaled = embeddings_local * local_score\n",
    "        embeddings_global_scaled = embeddings_global * global_score\n",
    "        embeddings_result = embeddings_local_scaled + embeddings_global_scaled\n",
    "        \n",
    "        self._loss_component = local_score_loss(local_logits)\n",
    "        \n",
    "        # batch_size x sequence_length x attention_heads * head_dim\n",
    "        return embeddings_result.view((batch_size, sequence_length, self.attention_heads * self.head_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ContextChoiceConstant(BaseContextChoice):\n",
    "    def __init__(self, attention_heads: int, embedding_dim: int, loss_k: float = LOCAL_LOSS_K) -> None:\n",
    "        super().__init__(attention_heads, embedding_dim, loss_k)\n",
    "        self.bias = nn.Parameter(torch.randn((self.attention_heads)))\n",
    "\n",
    "    def forward(self, embeddings_local: torch.FloatTensor, embeddings_global: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        batch_size, sequence_length, _ = embeddings_local.shape\n",
    "        # batch_size x sequence_length x attention_heads x head_dim\n",
    "        embeddings_local = embeddings_local.view((batch_size, sequence_length, self.attention_heads, -1))\n",
    "        embeddings_global = embeddings_global.view((batch_size, sequence_length, self.attention_heads, -1))\n",
    "        # 1 x 1 x attention_heads x 1\n",
    "        logits_local = self.bias.view((1, 1, self.attention_heads, 1))\n",
    "        scores_local = F.sigmoid(logits_local)\n",
    "        scores_global = 1 - scores_local\n",
    "        # batch_size x sequence_length x attention_heads x head_dim\n",
    "        embeddings_local_scaled = embeddings_local * scores_local\n",
    "        embeddings_global_scaled = embeddings_global * scores_global\n",
    "        embeddings_result = embeddings_local_scaled + embeddings_global_scaled\n",
    "        \n",
    "        self._loss_component = local_score_loss(logits_local)\n",
    "        \n",
    "        # batch_size x sequence_length x attention_heads * head_dim\n",
    "        return embeddings_result.view((batch_size, sequence_length, self.attention_heads * self.head_dim))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ContextChoiceLinear"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the \"bshd,hda->bsha\" einsum expression was generated with help of GPT-4 after I explained what I am going to do and gave it an example of for-loop + nn.Linear based example which do what I need - let's check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_initialize_einsum_classifier():\n",
    "    # Initializing the einsum version\n",
    "    classifier = ContextChoiceLinear(\n",
    "        attention_heads=16,\n",
    "        embedding_dim=16 * 8,\n",
    "    )\n",
    "    classifier.eval()\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_initialize_linear_classifiers(classifier):\n",
    "    # Initializing equal(?) linear classifiers\n",
    "    linears = []\n",
    "    for i in range(classifier.attention_heads):\n",
    "        layer = nn.Linear(in_features=classifier.head_dim, out_features=1)\n",
    "        layer.load_state_dict({\n",
    "            \"weight\": classifier.weights[i, :, :].transpose(0, 1),\n",
    "            \"bias\": classifier.biases[[i]],\n",
    "        })\n",
    "        layer.eval()\n",
    "        linears.append(layer)\n",
    "    return linears    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_apply_naive_linears(embeddings_local, embeddings_global, classifier, linears):\n",
    "    # apply \"naive\" version\n",
    "    embeddings_local = embeddings_local.view((\n",
    "        embeddings_local.shape[0],\n",
    "        embeddings_local.shape[1],\n",
    "        classifier.attention_heads,\n",
    "        classifier.head_dim\n",
    "    ))\n",
    "    embeddings_global = embeddings_global.view((\n",
    "        embeddings_global.shape[0],\n",
    "        embeddings_global.shape[1],\n",
    "        classifier.attention_heads,\n",
    "        classifier.head_dim\n",
    "    ))\n",
    "    embeddings_scored_all = []\n",
    "    for i, layer in enumerate(linears):\n",
    "        local_score = F.sigmoid(layer(embeddings_local[:, :, i, :]))\n",
    "        global_score = 1 - local_score\n",
    "        embeddings_scored = embeddings_local[:, :, i, :] * local_score + embeddings_global[:, :, i, :] * global_score\n",
    "        embeddings_scored = embeddings_scored.view((\n",
    "            embeddings_scored.shape[0],\n",
    "            embeddings_scored.shape[1],\n",
    "            1,\n",
    "            -1,\n",
    "        ))\n",
    "        embeddings_scored_all.append(embeddings_scored)\n",
    "    prediction_naive = torch.cat(embeddings_scored_all, dim=2)\n",
    "    prediction_naive = prediction_naive.view((\n",
    "        prediction_naive.shape[0],\n",
    "        prediction_naive.shape[1],\n",
    "        -1\n",
    "    ))\n",
    "    return prediction_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_einsum_classifier():\n",
    "    classifier = _test_initialize_einsum_classifier()\n",
    "    linears = _test_initialize_linear_classifiers(classifier)\n",
    "    # Generate embeddings\n",
    "    embeddings_local = torch.randn((2, 64, classifier.attention_heads * classifier.head_dim))\n",
    "    embeddings_global = torch.randn((2, 64, classifier.attention_heads * classifier.head_dim))\n",
    "    # Than apply einsum version\n",
    "    with torch.no_grad():\n",
    "        prediction_einsum = classifier(embeddings_local, embeddings_global)\n",
    "        prediction_naive = _test_apply_naive_linears(embeddings_local, embeddings_global, classifier, linears)\n",
    "    # And see if the computational difference is small\n",
    "    diff_max = (prediction_naive - prediction_einsum).abs().max()\n",
    "    assert diff_max < 1e-6\n",
    "    return diff_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.5367e-07)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs = []\n",
    "for i in range(100):\n",
    "    torch.manual_seed(42 + i)\n",
    "    diffs.append(_test_einsum_classifier())\n",
    "max(diffs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that sounds like a success. Every time the difference between \"naive\" method and GPT-4-help-made-einsum-method was less than 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
