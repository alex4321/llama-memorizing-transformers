{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex4321/anaconda3/envs/longdocchat/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Union\n",
    "from transformers.models.llama import LlamaModel\n",
    "from llama_memorizing_transformers.memorizing_block import MemorizingLlamaDecoderLayer\n",
    "from llama_memorizing_transformers.memory_collection import BaseMemoryCollection, CosineKnnMemoryCollection\n",
    "from llama_memorizing_transformers.context_choice import BaseContextChoice, ContextChoiceConstant, ContextChoiceLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from llama_4bit_wrapper import import_llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def replace_llama_layer_with_memory(model: LlamaModel,\n",
    "                                    layer_index: int,\n",
    "                                    context: BaseContextChoice,\n",
    "                                    memory: BaseMemoryCollection) -> LlamaModel:\n",
    "    original_layer = model.layers[layer_index]\n",
    "    new_layer = MemorizingLlamaDecoderLayer(\n",
    "        module=original_layer,\n",
    "        context_choice=context.to(model.device),\n",
    "        memory=memory,\n",
    "    )\n",
    "    model.layers[layer_index] = new_layer\n",
    "    model._memorizing_patch = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../vicuna-13b-GPTQ-4bit-128g\"):\n",
    "    !git clone \"https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g\"\n",
    "    !mv \"vicuna-13b-GPTQ-4bit-128g\" .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Triton implementation.\n"
     ]
    }
   ],
   "source": [
    "_, _, load_llama_model_4bit_low_ram, _, model_to_half, _, _, _, AMPWrapper = import_llama(\n",
    "    use_flash_attention=True,\n",
    "    use_xformers=False,\n",
    "    autograd_4bit_cuda=False,\n",
    "    autograd_4bit_triton=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The safetensors archive passed at ../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model in 3.54 seconds.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_llama_model_4bit_low_ram(\n",
    "    config_path=\"../vicuna-13b-GPTQ-4bit-128g/\",\n",
    "    model_path=\"../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\",\n",
    "    groupsize=128,\n",
    "    is_v1_model=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model = replace_llama_layer_with_memory(\n",
    "    model.model,\n",
    "    21,\n",
    "    ContextChoiceLinear(model.config.num_attention_heads, model.config.hidden_size),\n",
    "    CosineKnnMemoryCollection(1024, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted as Half.\n"
     ]
    }
   ],
   "source": [
    "model_to_half(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = AMPWrapper(model)\n",
    "wrapper.apply_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''I think the meaning of life is'''\n",
    "batch = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "batch = {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate():\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(inputs=batch[\"input_ids\"],\n",
    "                                do_sample=True,\n",
    "                                use_cache=False,\n",
    "                                repetition_penalty=1.1,\n",
    "                                max_new_tokens=128,\n",
    "                                temperature=0.9,\n",
    "                                top_p=0.95,\n",
    "                                top_k=40,\n",
    "                                return_dict_in_generate=True,\n",
    "                                output_attentions=False,\n",
    "                                output_hidden_states=False,\n",
    "                                output_scores=False)\n",
    "    result_text = tokenizer.decode(generated['sequences'].cpu().tolist()[0])\n",
    "    end = time.time()\n",
    "\n",
    "    print(result_text)\n",
    "    print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think the meaning of life is to be found in family, love and creating something positive with it and this is a book show it can be to be about that relationship that problem they whole thing, most concept idea story potential ending has to be biggest issue it story biggest challenge best song scene should have two problems solution first one I think you and movie best reason public and term end of the debate most t answer is to question new yok me next point will main thing could president last word movie same.\n",
      " R most important part most big the biggest idea message of the character story could world should be the future in world most people are more most key word point is probably he\n",
      "45.534175634384155\n"
     ]
    }
   ],
   "source": [
    "test_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
