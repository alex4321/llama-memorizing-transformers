{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex4321/anaconda3/envs/longdocchat/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from llama_4bit_wrapper import import_llama, lora_model_zeros_and_scales_to_half\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from llama_memorizing_transformers.memory_collection import CosineKnnMemoryCollection\n",
    "from llama_memorizing_transformers.context_choice import ContextChoiceLinear\n",
    "from llama_memorizing_transformers.model_wrapper import replace_llama_layer_with_memory\n",
    "from llama_memorizing_transformers.document_trainer import MemorizingLlamaDocumentTrainer\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "# Dataset\n",
    "DATASET_PATH = \"long-vicuna-set-lessgpt4all-vicuna13b-processed\"\n",
    "\n",
    "# Training procedure\n",
    "CONTEXT_LENGTH = 512\n",
    "CONTEXT_STEP = 256\n",
    "PRETRAIN_LENGTH = 2048\n",
    "PRETRAIN_DOCUMENTS = 2048\n",
    "\n",
    "# Model\n",
    "COSINE_KNN_MAX_TEMPORARY_BUFFER_SIZE = 1024\n",
    "REPLACE_LAYER = 21\n",
    "BASE_MODEL = \"../vicuna-13b-GPTQ-4bit-128g\"\n",
    "BASE_MODEL_WEIGHTS = \"../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\"\n",
    "\n",
    "USE_FP16 = True\n",
    "LR_PRETRAIN = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Triton implementation.\n"
     ]
    }
   ],
   "source": [
    "_, _, load_llama_model_4bit_low_ram, _, _, _, apply_gradient_checkpointing, _, _ = import_llama(\n",
    "    use_flash_attention=False,\n",
    "    use_xformers=False,\n",
    "    autograd_4bit_cuda=False,\n",
    "    autograd_4bit_triton=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;msg_prompter&gt; Can you write a short introduct...</td>\n",
       "      <td>[529, 7645, 29918, 14032, 29886, 357, 29958, 1...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;msg_assistant&gt; \"Monopsony\" refers to a market...</td>\n",
       "      <td>[529, 7645, 29918, 465, 22137, 29958, 376, 718...</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;msg_prompter&gt; Now explain it to a dog</td>\n",
       "      <td>[529, 7645, 29918, 14032, 29886, 357, 29958, 2...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;msg_assistant&gt; Monopsony is a market structur...</td>\n",
       "      <td>[529, 7645, 29918, 465, 22137, 29958, 2598, 45...</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;msg_prompter&gt; How can one fight back when a m...</td>\n",
       "      <td>[529, 7645, 29918, 14032, 29886, 357, 29958, 1...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      processed_text   \n",
       "0  <msg_prompter> Can you write a short introduct...  \\\n",
       "1  <msg_assistant> \"Monopsony\" refers to a market...   \n",
       "2             <msg_prompter> Now explain it to a dog   \n",
       "3  <msg_assistant> Monopsony is a market structur...   \n",
       "4  <msg_prompter> How can one fight back when a m...   \n",
       "\n",
       "                                           input_ids  length  \n",
       "0  [529, 7645, 29918, 14032, 29886, 357, 29958, 1...      50  \n",
       "1  [529, 7645, 29918, 465, 22137, 29958, 376, 718...     351  \n",
       "2  [529, 7645, 29918, 14032, 29886, 357, 29958, 2...      13  \n",
       "3  [529, 7645, 29918, 465, 22137, 29958, 2598, 45...     238  \n",
       "4  [529, 7645, 29918, 14032, 29886, 357, 29958, 1...      22  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_texts = pd.read_pickle(os.path.join(DATASET_PATH, \"texts.pkl\"))\n",
    "df_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indices</th>\n",
       "      <th>source</th>\n",
       "      <th>session_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>openassistant</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 3, 4]</td>\n",
       "      <td>openassistant</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 5, 6, 7]</td>\n",
       "      <td>openassistant</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 5, 6, 8]</td>\n",
       "      <td>openassistant</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 5, 6, 9]</td>\n",
       "      <td>openassistant</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        indices         source  session_length\n",
       "0     [0, 1, 2]  openassistant             414\n",
       "1     [0, 3, 4]  openassistant             310\n",
       "2  [0, 5, 6, 7]  openassistant             426\n",
       "3  [0, 5, 6, 8]  openassistant             595\n",
       "4  [0, 5, 6, 9]  openassistant             334"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_indices_train = pd.read_pickle(os.path.join(DATASET_PATH, \"indices-train.pkl\"))\n",
    "df_indices_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indices</th>\n",
       "      <th>source</th>\n",
       "      <th>session_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[82483, 82484]</td>\n",
       "      <td>openassistant</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[82483, 82485]</td>\n",
       "      <td>openassistant</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[82483, 82486]</td>\n",
       "      <td>openassistant</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[82487, 82488]</td>\n",
       "      <td>openassistant</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[82487, 82489, 82490, 82491]</td>\n",
       "      <td>openassistant</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        indices         source  session_length\n",
       "0                [82483, 82484]  openassistant             302\n",
       "1                [82483, 82485]  openassistant             218\n",
       "2                [82483, 82486]  openassistant              79\n",
       "3                [82487, 82488]  openassistant             561\n",
       "4  [82487, 82489, 82490, 82491]  openassistant             546"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_indices_validation = pd.read_pickle(os.path.join(DATASET_PATH, \"indices-validation.pkl\"))\n",
    "df_indices_validation.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The safetensors archive passed at ../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model in 3.45 seconds.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_llama_model_4bit_low_ram(\n",
    "    config_path=\"../vicuna-13b-GPTQ-4bit-128g/\",\n",
    "    model_path=\"../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\",\n",
    "    groupsize=128,\n",
    "    is_v1_model=False,\n",
    ")\n",
    "tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_choice = ContextChoiceLinear(model.config.num_attention_heads,\n",
    "                                     model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = CosineKnnMemoryCollection(COSINE_KNN_MAX_TEMPORARY_BUFFER_SIZE,\n",
    "                                   remember_until_position=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model = replace_llama_layer_with_memory(\n",
    "    model.model,\n",
    "    REPLACE_LAYER,\n",
    "    context_choice,\n",
    "    memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model = lora_model_zeros_and_scales_to_half(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Patch Applied For Block 0\n",
      "Forward Patch Applied For Block 1\n",
      "Forward Patch Applied For Block 2\n",
      "Forward Patch Applied For Block 3\n",
      "Forward Patch Applied For Block 4\n",
      "Forward Patch Applied For Block 5\n",
      "Forward Patch Applied For Block 6\n",
      "Forward Patch Applied For Block 7\n",
      "Forward Patch Applied For Block 8\n",
      "Forward Patch Applied For Block 9\n",
      "Forward Patch Applied For Block 10\n",
      "Forward Patch Applied For Block 11\n",
      "Forward Patch Applied For Block 12\n",
      "Forward Patch Applied For Block 13\n",
      "Forward Patch Applied For Block 14\n",
      "Forward Patch Applied For Block 15\n",
      "Forward Patch Applied For Block 16\n",
      "Forward Patch Applied For Block 17\n",
      "Forward Patch Applied For Block 18\n",
      "Forward Patch Applied For Block 19\n",
      "Forward Patch Applied For Block 20\n",
      "Forward Patch Applied For Block 21\n",
      "Forward Patch Applied For Block 22\n",
      "Forward Patch Applied For Block 23\n",
      "Forward Patch Applied For Block 24\n",
      "Forward Patch Applied For Block 25\n",
      "Forward Patch Applied For Block 26\n",
      "Forward Patch Applied For Block 27\n",
      "Forward Patch Applied For Block 28\n",
      "Forward Patch Applied For Block 29\n",
      "Forward Patch Applied For Block 30\n",
      "Forward Patch Applied For Block 31\n",
      "Forward Patch Applied For Block 32\n",
      "Forward Patch Applied For Block 33\n",
      "Forward Patch Applied For Block 34\n",
      "Forward Patch Applied For Block 35\n",
      "Forward Patch Applied For Block 36\n",
      "Forward Patch Applied For Block 37\n",
      "Forward Patch Applied For Block 38\n",
      "Forward Patch Applied For Block 39\n",
      "Var Wrapper Patch Applied\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c384310>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888f496e10>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c27f690>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f8890625c50>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888f469810>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c285a50>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c2652d0>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888f497550>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c2fd050>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c287710>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c2d6950>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c2d6cd0>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c249450>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c146790>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c146bd0>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c2d75d0>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c273ed0>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888f42ba50>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888f4af2d0>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c24a990>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c26edd0>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c314b10>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888f4acf90>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888f429410>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c121f90>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c35da10>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c285950>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c281510>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888f4ae390>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c266c50>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c2d6a90>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888fd0ee10>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c347750>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c271010>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f889d307dd0>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c286950>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c147e10>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c2c0810>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888f428d50>,\n",
       "  <alpaca_lora_4bit.gradient_checkpointing.NewForward at 0x7f888c16f1d0>],\n",
       " <alpaca_lora_4bit.gradient_checkpointing.VarWrapper at 0x7f888c146b90>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_gradient_checkpointing(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.config.use_cache = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenized texts checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110934</th>\n",
       "      <td>&lt;msg_prompter&gt; Below is an instruction that de...</td>\n",
       "      <td>[529, 7645, 29918, 14032, 29886, 357, 29958, 1...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74656</th>\n",
       "      <td>&lt;msg_prompter&gt; Куда падает ударение в слове \"т...</td>\n",
       "      <td>[529, 7645, 29918, 14032, 29886, 357, 29958, 7...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39950</th>\n",
       "      <td>&lt;msg_prompter&gt; А можешь побольше рассказать о ...</td>\n",
       "      <td>[529, 7645, 29918, 14032, 29886, 357, 29958, 1...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156432</th>\n",
       "      <td>&lt;msg_prompter&gt; Below is an instruction that de...</td>\n",
       "      <td>[529, 7645, 29918, 14032, 29886, 357, 29958, 1...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174184</th>\n",
       "      <td>&lt;msg_prompter&gt; Below is an instruction that de...</td>\n",
       "      <td>[529, 7645, 29918, 14032, 29886, 357, 29958, 1...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           processed_text   \n",
       "110934  <msg_prompter> Below is an instruction that de...  \\\n",
       "74656   <msg_prompter> Куда падает ударение в слове \"т...   \n",
       "39950   <msg_prompter> А можешь побольше рассказать о ...   \n",
       "156432  <msg_prompter> Below is an instruction that de...   \n",
       "174184  <msg_prompter> Below is an instruction that de...   \n",
       "\n",
       "                                                input_ids  length  \n",
       "110934  [529, 7645, 29918, 14032, 29886, 357, 29958, 1...      59  \n",
       "74656   [529, 7645, 29918, 14032, 29886, 357, 29958, 7...      23  \n",
       "39950   [529, 7645, 29918, 14032, 29886, 357, 29958, 1...      48  \n",
       "156432  [529, 7645, 29918, 14032, 29886, 357, 29958, 1...      72  \n",
       "174184  [529, 7645, 29918, 14032, 29886, 357, 29958, 1...      57  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_texts.sample(5, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110934    <s> <msg_prompter> Below is an instruction tha...\n",
       "74656     <s> <msg_prompter> Куда падает ударение в слов...\n",
       "39950     <s> <msg_prompter> А можешь побольше рассказат...\n",
       "156432    <s> <msg_prompter> Below is an instruction tha...\n",
       "174184    <s> <msg_prompter> Below is an instruction tha...\n",
       "Name: input_ids, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_texts.sample(5, random_state=RANDOM_STATE)[\"input_ids\"].apply(\n",
    "    lambda item: tokenizer.decode([tokenizer.bos_token_id] + list(item) + [tokenizer.eos_token_id])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "alpaca            125.0\n",
       "booksum          1311.5\n",
       "govreport        1548.0\n",
       "gpt4all           300.0\n",
       "openassistant     388.0\n",
       "qasper           1652.0\n",
       "Name: session_length, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_indices_train.loc[df_indices_train[\"session_length\"] <= PRETRAIN_LENGTH].groupby(\"source\")[\"session_length\"].quantile(0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(\n",
    "    lora_model.parameters(),\n",
    "    lr=LR_PRETRAIN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indices</th>\n",
       "      <th>source</th>\n",
       "      <th>session_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12096</th>\n",
       "      <td>[23183, 23189, 23190, 23195, 23196]</td>\n",
       "      <td>openassistant</td>\n",
       "      <td>1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286331</th>\n",
       "      <td>[795568, 795569]</td>\n",
       "      <td>gpt4all</td>\n",
       "      <td>972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150295</th>\n",
       "      <td>[244918, 244919]</td>\n",
       "      <td>gpt4all</td>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294713</th>\n",
       "      <td>[952816, 952817]</td>\n",
       "      <td>gpt4all</td>\n",
       "      <td>1005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355534</th>\n",
       "      <td>[301340, 301341]</td>\n",
       "      <td>gpt4all</td>\n",
       "      <td>874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    indices         source  session_length\n",
       "12096   [23183, 23189, 23190, 23195, 23196]  openassistant            1594\n",
       "286331                     [795568, 795569]        gpt4all             972\n",
       "150295                     [244918, 244919]        gpt4all             584\n",
       "294713                     [952816, 952817]        gpt4all            1005\n",
       "355534                     [301340, 301341]        gpt4all             874"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_indices_pretrain = df_indices_train\\\n",
    "    .loc[(df_indices_train[\"session_length\"] <= PRETRAIN_LENGTH) & (df_indices_train[\"session_length\"] >CONTEXT_LENGTH)]\\\n",
    "    .sample(PRETRAIN_DOCUMENTS, random_state=RANDOM_STATE)\n",
    "df_indices_pretrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "_batch_counter = 0\n",
    "\n",
    "def batch_counter_update() -> int:\n",
    "    global _batch_counter\n",
    "    _batch_counter += 1\n",
    "    return _batch_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2048/2048 [7:46:25<00:00, 13.66s/it]  \n"
     ]
    }
   ],
   "source": [
    "log_writer = SummaryWriter(\"long-vicuna--pretrain--tensorboard\")\n",
    "document_trainer = MemorizingLlamaDocumentTrainer(\n",
    "    model=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    memory=memory,\n",
    "    tokens_per_chunk=CONTEXT_LENGTH,\n",
    "    tokens_step=CONTEXT_STEP,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=None,\n",
    "    accumulate_gradients=1,\n",
    "    float16=USE_FP16,\n",
    "    train_callback=lambda document_index, document_batch, loss: log_writer.add_scalar(\"Loss/pretrain\", loss, batch_counter_update()),\n",
    "    eval_callback=None,\n",
    ")\n",
    "for i, indices in enumerate(tqdm(df_indices_pretrain[\"indices\"])):\n",
    "    main_document_index = indices[0]\n",
    "    rest_session_index = indices[1:]\n",
    "    document_tokens = torch.LongTensor(df_texts.loc[main_document_index, \"input_ids\"].astype(np.int32))\n",
    "    rest_session_tokens = torch.cat([\n",
    "        torch.LongTensor(array.astype(np.int32))\n",
    "        for array in df_texts.loc[rest_session_index, \"input_ids\"]\n",
    "    ]).view((1, -1))\n",
    "    document_trainer.train_document(\n",
    "        document_tokens=document_tokens,\n",
    "        prompt_tokens=rest_session_tokens,\n",
    "        sample_weight=1.0,\n",
    "        callback_kwargs={\n",
    "            \"document_index\": i,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lora_model.state_dict(), \"long-vicuna--pretrain--state-dict.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longdocchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
