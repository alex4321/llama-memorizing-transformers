{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp document_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex4321/anaconda3/envs/longdocchat/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from math import ceil\n",
    "from typing import List, Union, Tuple, Iterable, Any, Dict, Callable\n",
    "import torch\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from peft import PeftModelForCausalLM\n",
    "from transformers.models.llama import LlamaForCausalLM, LlamaTokenizer, LlamaTokenizerFast\n",
    "from llama_memorizing_transformers.memory_collection import BaseMemoryCollection\n",
    "from llama_memorizing_transformers.context_choice import BaseContextChoice\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MemorizingLlamaDocumentTrainer:\n",
    "    def __init__(self, model: Union[LlamaForCausalLM, PeftModelForCausalLM],\n",
    "                 context_choice: BaseContextChoice,\n",
    "                 tokenizer: Union[LlamaTokenizerFast, LlamaTokenizer],\n",
    "                 memory: BaseMemoryCollection,\n",
    "                 tokens_per_chunk: int,\n",
    "                 tokens_step: int,\n",
    "                 optimizer: Optimizer,\n",
    "                 scheduler: Union[LambdaLR, None],\n",
    "                 accumulate_gradients: int,\n",
    "                 float16: bool,\n",
    "                 train_callback: Union[callable, None],\n",
    "                 eval_callback: Union[callable, None]):\n",
    "        if isinstance(model, LlamaForCausalLM):\n",
    "            assert hasattr(model.model, \"_memorizing_patch\")\n",
    "        elif isinstance(model, PeftModelForCausalLM):\n",
    "            assert hasattr(model.base_model.model.model, \"_memorizing_patch\")\n",
    "        else:\n",
    "            raise TypeError(\"Unknown model type\")\n",
    "        self.llama = model\n",
    "        self.context_choice = context_choice\n",
    "        self.tokenizer = tokenizer\n",
    "        self.memory = memory\n",
    "        self.tokens_per_chunk = tokens_per_chunk\n",
    "        self.tokens_step = tokens_step\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.accumulate_gradients = accumulate_gradients\n",
    "        self.float16 = float16\n",
    "        self.train_callback = train_callback\n",
    "        self.eval_callback = eval_callback\n",
    "\n",
    "    def _rearrange_tokens(self, document_tokens: torch.LongTensor, prompt_tokens: torch.LongTensor) -> Iterable[torch.LongTensor]:\n",
    "        for i in range(prompt_tokens.shape[0]):\n",
    "            prompt_tokens_processed = torch.cat((document_tokens, prompt_tokens[i]), dim=0).view((1, -1))\n",
    "            yield prompt_tokens_processed\n",
    "\n",
    "    def _split_token_sequences(self, token_sequence: torch.LongTensor) -> List[torch.LongTensor]:\n",
    "        _, seq_len = token_sequence.shape\n",
    "        start_index = 0\n",
    "        sub_sequences = []\n",
    "        while start_index < seq_len:\n",
    "            sub_sequence = token_sequence[:, start_index : start_index + self.tokens_per_chunk]\n",
    "            if sub_sequence.shape[1] > 0:\n",
    "                sub_sequences.append(sub_sequence)\n",
    "            start_index += self.tokens_step\n",
    "        return sub_sequences\n",
    "    \n",
    "    def _get_train_block_tokens(self, document_tokens: torch.LongTensor, prompt_tokens: torch.LongTensor) -> Iterable[Tuple[torch.LongTensor, torch.LongTensor]]:\n",
    "        assert len(document_tokens.shape) == 1, \"document tokens should be 1d array\"\n",
    "        assert len(prompt_tokens.shape) == 2, \"prompt tokens should be 2d array\"\n",
    "        self.memory.remember_until_position = document_tokens.shape[0]\n",
    "        for item_tokens in self._rearrange_tokens(document_tokens, prompt_tokens):\n",
    "            item_prompt_tokens = self._split_token_sequences(item_tokens[:, :-1])\n",
    "            item_labels_tokens = self._split_token_sequences(item_tokens[:, 1:])\n",
    "            self.memory.reset()\n",
    "            for block_prompt_tokens, block_label_tokens in zip(item_prompt_tokens, item_labels_tokens):\n",
    "                 yield block_prompt_tokens, block_label_tokens\n",
    "\n",
    "    @property\n",
    "    def _vocab_size(self) -> int:\n",
    "        if self.tokenizer.pad_token_id is not None:\n",
    "            return self.tokenizer.vocab_size + 1\n",
    "        return self.tokenizer.vocab_size\n",
    "\n",
    "    def _get_losses(self, document_tokens: torch.LongTensor, prompt_tokens: torch.LongTensor, sample_weight: float) -> \\\n",
    "        Iterable[Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]]:\n",
    "        def _inner(block_prompt_tokens: torch.LongTensor, block_label_tokens: torch.LongTensor) -> torch.FloatTensor:\n",
    "            block_prompt_tokens = block_prompt_tokens.to(self.llama.device)\n",
    "            block_label_tokens = block_label_tokens.to(self.llama.device)\n",
    "            \n",
    "            block_attention_mask = (block_prompt_tokens != self.tokenizer.pad_token_id).float()\n",
    "            block_label_mask = (block_label_tokens != self.tokenizer.pad_token_id ).float()\n",
    "            block_label_tokens = ((block_label_mask * block_label_tokens) + (1.0 - block_label_mask) * (-100)).long()\n",
    "            model_forward_pass = self.llama(\n",
    "                input_ids=block_prompt_tokens.to(self.llama.device),\n",
    "                attention_mask=block_attention_mask.to(self.llama.device),\n",
    "                labels=block_label_tokens,\n",
    "                return_dict=True\n",
    "            )\n",
    "            logits = model_forward_pass.logits\n",
    "\n",
    "            logits_flatten = logits.view((-1, self._vocab_size))\n",
    "            labels_flatten = block_label_tokens.view((-1,))\n",
    "            lm_loss = cross_entropy(input=logits_flatten, target=labels_flatten) * sample_weight\n",
    "            context_choice_loss = self.context_choice.get_loss_component() * sample_weight\n",
    "            loss = lm_loss + context_choice_loss\n",
    "            return loss, context_choice_loss, lm_loss\n",
    "\n",
    "        loss = 0\n",
    "        loss_context = 0\n",
    "        loss_lm = 0\n",
    "        for block_prompt_tokens, block_label_tokens in self._get_train_block_tokens(document_tokens, prompt_tokens):\n",
    "            del loss, loss_context, loss_lm\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            if self.float16:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss, loss_context, loss_lm = _inner(block_prompt_tokens, block_label_tokens)\n",
    "            else:\n",
    "                loss, loss_context, loss_lm = _inner(block_prompt_tokens, block_label_tokens)\n",
    "            yield loss, loss_context, loss_lm\n",
    "\n",
    "    def train_document(self, document_tokens: torch.LongTensor, prompt_tokens: torch.LongTensor, sample_weight: float, callback_kwargs: Dict[str, Any]):\n",
    "        self.llama.train()\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "        if self.float16:\n",
    "            scaler = torch.cuda.amp.grad_scaler.GradScaler()\n",
    "        else:\n",
    "            scaler = None\n",
    "        for batch, losses in enumerate(self._get_losses(document_tokens, prompt_tokens, sample_weight)):\n",
    "            loss, loss_context, loss_lm = losses\n",
    "            if self.float16:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            if batch % self.accumulate_gradients == 0:\n",
    "                if self.float16:\n",
    "                    scaler.step(self.optimizer)\n",
    "                else:\n",
    "                    self.optimizer.step()\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                if self.float16:\n",
    "                    scaler.update()\n",
    "            batch_callback_kwargs = dict(callback_kwargs, document_batch=batch,\n",
    "                                         loss=loss.item(),\n",
    "                                         loss_lm=loss_lm.item(),\n",
    "                                         loss_context=loss_context.item())\n",
    "            del loss, loss_context, loss_lm\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            if self.train_callback:\n",
    "                self.train_callback(**batch_callback_kwargs)\n",
    "\n",
    "    def eval_document(self, document_tokens: torch.LongTensor, prompt_tokens: torch.LongTensor, sample_weight: float, callback_kwargs: Dict[str, Any]):\n",
    "        self.llama.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch, losses in enumerate(self._get_losses(document_tokens, prompt_tokens, sample_weight)):\n",
    "                loss, loss_context, loss_lm = losses\n",
    "                loss = loss.item()\n",
    "                loss_context = loss_context.item()\n",
    "                loss_lm = loss_lm.item()\n",
    "                batch_callback_kwargs = dict(callback_kwargs, document_batch=batch,\n",
    "                                             loss=loss,\n",
    "                                             loss_lm=loss_lm,\n",
    "                                             loss_context=loss_context)\n",
    "                if self.eval_callback:\n",
    "                    self.eval_callback(**batch_callback_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
