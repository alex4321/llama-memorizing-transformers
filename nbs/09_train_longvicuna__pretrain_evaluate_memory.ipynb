{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "from llama_4bit_wrapper import import_llama, lora_model_zeros_and_scales_to_half\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from llama_memorizing_transformers.memory_collection import CosineKnnMemoryCollection\n",
    "from llama_memorizing_transformers.context_choice import ContextChoiceLinear\n",
    "from llama_memorizing_transformers.model_wrapper import replace_llama_layer_with_memory\n",
    "from llama_memorizing_transformers.document_trainer import MemorizingLlamaDocumentTrainer\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from torch.nn.functional import softmax\n",
    "import gc\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"09_train_longvicuna__pretrain_api_key.txt\", \"r\") as src:\n",
    "    openai.api_key = src.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "# Dataset\n",
    "DATASET_PATH = \"long-vicuna-set-lessgpt4all-vicuna13b-processed\"\n",
    "\n",
    "# Training procedure\n",
    "CONTEXT_LENGTH = 512\n",
    "CONTEXT_STEP = 256\n",
    "PRETRAIN_LENGTH = 1024\n",
    "# Model\n",
    "COSINE_KNN_MAX_TEMPORARY_BUFFER_SIZE = 1024\n",
    "REPLACE_LAYER = 21\n",
    "BASE_MODEL = \"../vicuna-13b-GPTQ-4bit-128g\"\n",
    "BASE_MODEL_WEIGHTS = \"../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\"\n",
    "\n",
    "USE_FP16 = True\n",
    "LR_PRETRAIN = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, load_llama_model_4bit_low_ram, _, model_to_half, _, _, _, AMPWrapper = import_llama(\n",
    "    use_flash_attention=False,\n",
    "    use_xformers=False,\n",
    "    autograd_4bit_cuda=False,\n",
    "    autograd_4bit_triton=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate memory-testing facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_memoryset_item():\n",
    "    initial_message = \"I am generating a dataset to evaluate other AI system memory. To do it I need your help.\\n\" + \\\n",
    "        \"Write some fact (one) about one fake person. You should mention this person name inside this fact. You shouldn't mention it's fake person.\"\n",
    "    question_message = \"Now write some question like 'What do we know about John Smith'.\\n\" + \\\n",
    "        \"Just replace John Smith with the name from the previously generated fact. Do not give any more hints.\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": initial_message}\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "    fact = response.choices[0].message.content\n",
    "    response = openai.ChatCompletion.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": initial_message},\n",
    "            {\"role\": \"assistant\", \"content\": fact},\n",
    "            {\"role\": \"user\", \"content\": question_message}\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    question = response.choices[0].message.content\n",
    "\n",
    "    return fact, question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_memoryset_facts(max_count):\n",
    "    result = []\n",
    "    for _ in tqdm(range(max_count)):\n",
    "        try:\n",
    "            fact, question = generate_memoryset_item()\n",
    "            result.append({\"fact\": fact, \"question\": question})\n",
    "        except:\n",
    "            pass\n",
    "    return pd.DataFrame.from_records(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"09_train_longvicuna__pretrain_memoryset_facts.pkl\"):\n",
    "    df_memoryset_facts = generate_memoryset_facts(100)\n",
    "    df_memoryset_facts = df_memoryset_facts.loc[df_memoryset_facts[\"question\"].str.len() <= 60]\n",
    "    df_memoryset_facts.to_pickle(\"09_train_longvicuna__pretrain_memoryset_facts.pkl\")\n",
    "else:\n",
    "    df_memoryset_facts = pd.read_pickle(\"09_train_longvicuna__pretrain_memoryset_facts.pkl\")\n",
    "df_memoryset_facts.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_llama_model_4bit_low_ram(\n",
    "    config_path=BASE_MODEL,\n",
    "    model_path=BASE_MODEL_WEIGHTS,\n",
    "    groupsize=128,\n",
    "    is_v1_model=False,\n",
    ")\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "context_choice = ContextChoiceLinear(model.config.num_attention_heads,\n",
    "                                     model.config.hidden_size)\n",
    "memory = CosineKnnMemoryCollection(COSINE_KNN_MAX_TEMPORARY_BUFFER_SIZE,\n",
    "                                   remember_until_position=0)\n",
    "model.model = replace_llama_layer_with_memory(\n",
    "    model.model,\n",
    "    REPLACE_LAYER,\n",
    "    context_choice,\n",
    "    memory,\n",
    ")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model = lora_model_zeros_and_scales_to_half(lora_model)\n",
    "lora_model.config.use_cache = False\n",
    "wrapper = AMPWrapper(lora_model)\n",
    "wrapper.apply_forward()\n",
    "wrapper.apply_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.cpu()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.load_state_dict(torch.load(\n",
    "    \"long-vicuna--pretrain--state-dict--checkpoint-500.pth\",\n",
    "    map_location=torch.device(\"cpu\")\n",
    "))\n",
    "lora_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(lora_model, memory, input_ids):\n",
    "    lora_model.eval()\n",
    "    memory.reset()\n",
    "    memory.remember_until_position = input_ids.shape[1]\n",
    "    start = 0\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            block_input_ids = input_ids[:, start:start + CONTEXT_LENGTH].to(lora_model.device)\n",
    "            if block_input_ids.shape[1]:\n",
    "                lora_model(input_ids=block_input_ids)\n",
    "            else:\n",
    "                break\n",
    "            start += CONTEXT_STEP\n",
    "    last_block_ids = input_ids[:, -CONTEXT_STEP:].to(lora_model.device)\n",
    "    generated = lora_model.generate(inputs=last_block_ids,\n",
    "                               do_sample=True,\n",
    "                               use_cache=False,\n",
    "                               repetition_penalty=1.1,\n",
    "                               max_new_tokens=100,\n",
    "                               temperature=0.9,\n",
    "                               top_p=0.95,\n",
    "                               top_k=40,\n",
    "                               return_dict_in_generate=True,\n",
    "                               output_attentions=False,\n",
    "                               output_hidden_states=False,\n",
    "                               output_scores=False)\n",
    "    return generated.sequences[0][-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = df_memoryset_facts[\"question\"].values[0]\n",
    "text = \"<msg_prompter> \" + \"\\n\\n\".join(df_memoryset_facts[\"fact\"]) + \"\\n\\n<msg_prompter> Now answer the following question: \" + \\\n",
    "     question + \"\\n<msg_assistant> \"\n",
    "input_ids = torch.LongTensor([tokenizer(text)[\"input_ids\"]])\n",
    "\n",
    "generated = generate_response(lora_model, memory, input_ids)\n",
    "print(question)\n",
    "print(tokenizer.decode(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(df_memoryset_facts.loc[df_memoryset_facts[\"fact\"].str.lower().str.contains(\"samantha\"), \"fact\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memoryset_facts = df_memoryset_facts.sample(len(df_memoryset_facts), random_state=42)\n",
    "\n",
    "question = df_memoryset_facts[\"question\"].values[0]\n",
    "text = \"<msg_prompter> \" + \"\\n\\n\".join(df_memoryset_facts[\"fact\"]) + \"\\n\\n<msg_prompter> Now answer the following question: \" + \\\n",
    "     question + \"\\n<msg_assistant> \"\n",
    "input_ids = torch.LongTensor([tokenizer(text)[\"input_ids\"]])\n",
    "\n",
    "generated = generate_response(lora_model, memory, input_ids)\n",
    "print(question)\n",
    "print(tokenizer.decode(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(df_memoryset_facts.loc[df_memoryset_facts[\"fact\"].str.lower().str.contains(\"michael\"), \"fact\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memoryset_facts = df_memoryset_facts.sample(len(df_memoryset_facts), random_state=42)\n",
    "\n",
    "question = df_memoryset_facts[\"question\"].values[0]\n",
    "text = \"<msg_prompter> \" + \"\\n\\n\".join(df_memoryset_facts[\"fact\"]) + \"\\n\\n<msg_prompter> Now answer the following question: \" + \\\n",
    "     question + \"\\n<msg_assistant> \"\n",
    "input_ids = torch.LongTensor([tokenizer(text)[\"input_ids\"]])\n",
    "\n",
    "generated = generate_response(lora_model, memory, input_ids)\n",
    "print(question)\n",
    "print(tokenizer.decode(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(df_memoryset_facts.loc[df_memoryset_facts[\"fact\"].str.lower().str.contains(\"sophie\"), \"fact\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memoryset_facts = df_memoryset_facts.sample(len(df_memoryset_facts), random_state=42)\n",
    "\n",
    "question = df_memoryset_facts[\"question\"].values[0]\n",
    "text = \"<msg_prompter> \" + \"\\n\\n\".join(df_memoryset_facts[\"fact\"]) + \"\\n\\n<msg_prompter> Now answer the following question: \" + \\\n",
    "     question + \"\\n<msg_assistant> \"\n",
    "input_ids = torch.LongTensor([tokenizer(text)[\"input_ids\"]])\n",
    "\n",
    "generated = generate_response(lora_model, memory, input_ids)\n",
    "print(question)\n",
    "print(tokenizer.decode(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(df_memoryset_facts.loc[df_memoryset_facts[\"fact\"].str.lower().str.contains(\"john\"), \"fact\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
