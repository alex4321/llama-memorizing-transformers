# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_context_choice.ipynb.

# %% auto 0
__all__ = ['LOCAL_LOSS_K', 'local_score_loss', 'BaseContextChoice', 'ContextChoiceLinear', 'ContextChoiceConstant']

# %% ../nbs/00_context_choice.ipynb 2
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Union

# %% ../nbs/00_context_choice.ipynb 3
LOCAL_LOSS_K = 1.0

# %% ../nbs/00_context_choice.ipynb 4
def local_score_loss(local_score: torch.FloatTensor) -> torch.FloatTensor:
    targets = torch.zeros(local_score.shape, dtype=local_score.dtype, device=local_score.device)
    return F.binary_cross_entropy_with_logits(local_score, targets)

# %% ../nbs/00_context_choice.ipynb 5
class BaseContextChoice(nn.Module):
    """
    Base class for every context choice method.
    Basically each of these things is some kind of weighted average between the local context and global context.
    """
    def __init__(self, attention_heads: int, embedding_dim: int, loss_k: float = LOCAL_LOSS_K) -> None:
        """
        Initializer.
        :param attention_heads: how much attention heads each block of the original transformer have
        :param embedding_dim: how much embedding dimensions each block of the original transformer have
        """
        assert attention_heads > 0
        assert embedding_dim > 0
        assert embedding_dim % attention_heads == 0
        super(BaseContextChoice, self).__init__()
        self.attention_heads = attention_heads
        self.embedding_dim = embedding_dim
        self.head_dim = embedding_dim // attention_heads
        self._loss_component = 0
        self.loss_k = loss_k
    
    def get_loss_component(self) -> Union[torch.FloatTensor, float]:
        result = self._loss_component * self.loss_k
        self._loss_component = None
        return result
    
    def forward(self, embeddings_local: torch.FloatTensor, embeddings_global: torch.FloatTensor) -> torch.FloatTensor:
        """
        Apply the weighted average between embeddings_local and embeddings_global.
        """
        raise NotImplementedError("Each BaseContextChoice subclass must define their own forward method")

# %% ../nbs/00_context_choice.ipynb 6
class ContextChoiceLinear(BaseContextChoice):
    def __init__(self, attention_heads: int, embedding_dim: int, loss_k: float = LOCAL_LOSS_K) -> None:
        super(ContextChoiceLinear, self).__init__(attention_heads, embedding_dim, loss_k)
        self.weights = nn.Parameter(torch.randn((self.attention_heads, self.head_dim, 1)))
        self.biases = nn.Parameter(torch.randn((self.attention_heads,)))

    def forward(self, embeddings_local: torch.FloatTensor, embeddings_global: torch.FloatTensor) -> torch.FloatTensor:
        batch_size, sequence_length, _ = embeddings_local.shape
        # batch_size x sequence_length x attention_heads x head_dim
        embeddings_local = embeddings_local.view((batch_size, sequence_length, self.attention_heads, -1))
        embeddings_global = embeddings_global.view((batch_size, sequence_length, self.attention_heads, -1))
        # batch_size x sequence_length x attention_heads x 1
        # b - batch size
        # s - sequence length
        # h - attention heads
        # d - head dim
        # a - 1
        local_logits = torch.einsum("bshd,hda->bsha", embeddings_local, self.weights) + self.biases.view((1, 1, self.attention_heads, 1))    
        local_score = F.sigmoid(local_logits)
        global_score = 1 - local_score
        # batch_size x sequence_length x attention_heads x head_dim
        embeddings_local_scaled = embeddings_local * local_score
        embeddings_global_scaled = embeddings_global * global_score
        embeddings_result = embeddings_local_scaled + embeddings_global_scaled
        
        self._loss_component = local_score_loss(local_logits)
        
        # batch_size x sequence_length x attention_heads * head_dim
        return embeddings_result.view((batch_size, sequence_length, self.attention_heads * self.head_dim))

# %% ../nbs/00_context_choice.ipynb 7
class ContextChoiceConstant(BaseContextChoice):
    def __init__(self, attention_heads: int, embedding_dim: int, loss_k: float = LOCAL_LOSS_K) -> None:
        super().__init__(attention_heads, embedding_dim, loss_k)
        self.bias = nn.Parameter(torch.randn((self.attention_heads)))

    def forward(self, embeddings_local: torch.FloatTensor, embeddings_global: torch.FloatTensor) -> torch.FloatTensor:
        batch_size, sequence_length, _ = embeddings_local.shape
        # batch_size x sequence_length x attention_heads x head_dim
        embeddings_local = embeddings_local.view((batch_size, sequence_length, self.attention_heads, -1))
        embeddings_global = embeddings_global.view((batch_size, sequence_length, self.attention_heads, -1))
        # 1 x 1 x attention_heads x 1
        logits_local = self.bias.view((1, 1, self.attention_heads, 1))
        scores_local = F.sigmoid(logits_local)
        scores_global = 1 - scores_local
        # batch_size x sequence_length x attention_heads x head_dim
        embeddings_local_scaled = embeddings_local * scores_local
        embeddings_global_scaled = embeddings_global * scores_global
        embeddings_result = embeddings_local_scaled + embeddings_global_scaled
        
        self._loss_component = local_score_loss(logits_local)
        
        # batch_size x sequence_length x attention_heads * head_dim
        return embeddings_result.view((batch_size, sequence_length, self.attention_heads * self.head_dim))
